{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3_gsrJlqThn"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRIfHWFvE8hv"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7YFeYxqDKyP"
   },
   "outputs": [],
   "source": [
    "#copy reduced mnist data from drive and unzip it\n",
    "!unzip /content/drive/MyDrive/ML-Assign-4/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TPkBbHum59b",
    "outputId": "403955b6-8de8-4bf2-d38a-e2d1490a94d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob('/content/Reduced MNIST Data/Reduced Testing data/*/*')),len(glob.glob('/content/Reduced MNIST Data/Reduced Trainging data/*/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "509IoijzKDeo"
   },
   "source": [
    "### Training Le-Net-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aeEensY_nSHC"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import glob\n",
    "import cv2\n",
    "import tqdm\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "#setting random seed\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  g = torch.Generator()\n",
    "  g.manual_seed(seed)\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YdL4upMxnO0-"
   },
   "outputs": [],
   "source": [
    "#defining network\n",
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=5*5*16, out_features=120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "def initialize_weights(m):\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "      nn.init.xavier_uniform_(m.weight.data)\n",
    "      if m.bias is not None:\n",
    "          nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.Linear):\n",
    "      nn.init.xavier_uniform_(m.weight.data)\n",
    "      nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c5myvDm7oRTn"
   },
   "outputs": [],
   "source": [
    "#defining data class\n",
    "class data_set(Dataset):\n",
    "    def __init__(self,test=False):\n",
    "        if test:\n",
    "          self.imgs_dir = glob.glob('/content/Reduced MNIST Data/Reduced Testing data/*/*')\n",
    "        else:\n",
    "          self.imgs_dir = glob.glob('/content/Reduced MNIST Data/Reduced Trainging data/*/*')\n",
    "        self.transform_data = transforms.Compose([transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_dir)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        img_dir = self.imgs_dir[index]\n",
    "        label = int(img_dir.split('/')[-2])\n",
    "        img = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        img = self.transform_data(img)\n",
    "        return img,label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ALAtoVS_41jI"
   },
   "outputs": [],
   "source": [
    "#early stopping class\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f7I6TrI9VDG",
    "outputId": "d85dc4e7-9af0-41f1-bb23-c0a016cf5422",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch :1/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.5433745\n",
      "validation loss:  0.25939193\n",
      "---------- Epoch :2/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.2019528\n",
      "validation loss:  0.13938366\n",
      "---------- Epoch :3/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.14577691\n",
      "validation loss:  0.14690898\n",
      "---------- Epoch :4/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.11880944\n",
      "validation loss:  0.19681554\n",
      "---------- Epoch :5/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.14485052\n",
      "validation loss:  0.18871187\n",
      "---------- Epoch :6/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.1250468\n",
      "validation loss:  0.17930046\n",
      "---------- Epoch :7/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.06043309\n",
      "validation loss:  0.14658497\n",
      "---------- Epoch :8/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.039323173\n",
      "validation loss:  0.188017\n",
      "---------- Epoch :9/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.033132978\n",
      "validation loss:  0.16482964\n",
      "---------- Epoch :10/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.028692828\n",
      "validation loss:  0.17899065\n",
      "---------- Epoch :11/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.012011669\n",
      "validation loss:  0.1449328\n",
      "---------- Epoch :12/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0043223905\n",
      "validation loss:  0.16446961\n",
      "---------- Epoch :13/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.002453527\n",
      "validation loss:  0.17063436\n",
      "---------- Epoch :14/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0016366807\n",
      "validation loss:  0.17605104\n",
      "---------- Epoch :15/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.0012499182\n",
      "validation loss:  0.17883568\n",
      "----------  Early stopping  ----------\n",
      "training time:  46.1117  s\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#hyper-parameters values\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(0)\n",
    "\n",
    "num_epochs = 80\n",
    "lr = 0.02\n",
    "weight_decay = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "#model, dataloader, optimizer, scheduler, cost function, early stopping\n",
    "model = LeNet5()\n",
    "model.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,factor=0.5)\n",
    "\n",
    "DataSet = data_set()\n",
    "train_DataSet, val_DataSet = random_split(DataSet,[8000,2000])\n",
    "train_loader = DataLoader(train_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "val_loader = DataLoader(val_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.15)\n",
    "\n",
    "#training loop\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "stop_epoch = num_epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "  print('-'*10,f\"Epoch :{epoch+1}/{num_epochs}\",'-'*10)\n",
    "  print('learning rate: ',optimizer.param_groups[0][\"lr\"])\n",
    "  #train\n",
    "  losses=[]\n",
    "  model.train()\n",
    "  for imgs,labels in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #forward\n",
    "    preds = model(imgs)\n",
    "    #loss and backward\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(preds,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.detach().cpu())\n",
    "  print('train loss: ',np.mean(losses))\n",
    "  train_loss_epochs.append(np.mean(losses))\n",
    "  #validation\n",
    "  val_losses=[]\n",
    "  model.eval()\n",
    "  for imgs,labels in val_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    preds = model(imgs)\n",
    "    val_loss = loss_fn(preds,labels)\n",
    "    val_losses.append(val_loss.detach().cpu())\n",
    "  scheduler.step(np.mean(val_losses))\n",
    "  print('validation loss: ',np.mean(val_losses))\n",
    "  val_loss_epochs.append(np.mean(val_losses))\n",
    "\n",
    "  early_stopping(np.mean(losses), np.mean(val_losses))\n",
    "  if early_stopping.early_stop:\n",
    "    print('-'*10,\" Early stopping \",'-'*10)\n",
    "    stop_epoch = epoch + 1\n",
    "    break\n",
    "\n",
    "train_time = time.time() - startTime\n",
    "print('training time: ',round(train_time,4),' s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "6OORggAJDB1N",
    "outputId": "375d6a31-6689-439e-e44e-50be0f12337c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd95c6f3090>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss\n",
    "plt.plot(range(0,stop_epoch),train_loss_epochs)\n",
    "plt.plot(range(0,stop_epoch),val_loss_epochs)\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouvlW4mr9VHM",
    "outputId": "f7ed3978-a471-4869-9e9a-c69a42af7b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  98.1 %\n",
      "processing time:  0.3725  s\n"
     ]
    }
   ],
   "source": [
    "#tesing\n",
    "startTime = time.time()\n",
    "\n",
    "DataSet = data_set(test=True)\n",
    "len_data = DataSet.__len__()\n",
    "dataloader = DataLoader(DataSet, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "\n",
    "true_preds_tot = 0\n",
    "for imgs,labels in dataloader:\n",
    "  if torch.cuda.is_available():\n",
    "    imgs = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  preds = model(imgs)\n",
    "  true_preds = (torch.max(preds,axis=1).indices == labels).sum()\n",
    "  true_preds_tot += true_preds\n",
    "print('test accuracy: ',100*(true_preds_tot.cpu().numpy()/len_data),'%')\n",
    "test_time = time.time() - startTime\n",
    "print('processing time: ',round(test_time,4),' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHzWSJkaLLV5"
   },
   "source": [
    "### Training 3 variants of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qF7zBCN6LKkc"
   },
   "outputs": [],
   "source": [
    "#re-defining network\n",
    "\n",
    "#BatchNorm2d\n",
    "class VarNet1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VarNet1, self).__init__()\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=5*5*16, out_features=120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "#adding dropout\n",
    "class VarNet2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VarNet2, self).__init__()\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            torch.nn.Dropout(p=0.75),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            torch.nn.Dropout(p=0.75),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=5*5*16, out_features=120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "#using maxpool\n",
    "class VarNet3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VarNet3, self).__init__()\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=16*5*5, out_features=120),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=120, out_features=84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7a--0_S-brI"
   },
   "source": [
    "#### Variant 1: Using batch norm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaZ9b4icLKpb",
    "outputId": "9c4774b7-1a47-40e9-94c8-e2639f996a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch :1/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.4914384\n",
      "validation loss:  0.14125866\n",
      "---------- Epoch :2/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.16152617\n",
      "validation loss:  0.10593723\n",
      "---------- Epoch :3/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.10748588\n",
      "validation loss:  0.16183275\n",
      "---------- Epoch :4/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.09713488\n",
      "validation loss:  0.11033717\n",
      "---------- Epoch :5/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.07802499\n",
      "validation loss:  0.11847355\n",
      "---------- Epoch :6/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.07324106\n",
      "validation loss:  0.13768819\n",
      "---------- Epoch :7/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.029728843\n",
      "validation loss:  0.101703666\n",
      "---------- Epoch :8/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.0147679625\n",
      "validation loss:  0.08032896\n",
      "---------- Epoch :9/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.015862854\n",
      "validation loss:  0.12675406\n",
      "---------- Epoch :10/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.01725586\n",
      "validation loss:  0.08703883\n",
      "---------- Epoch :11/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.020871071\n",
      "validation loss:  0.10370174\n",
      "---------- Epoch :12/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.031226903\n",
      "validation loss:  0.10012075\n",
      "---------- Epoch :13/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.00856716\n",
      "validation loss:  0.0954434\n",
      "---------- Epoch :14/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0033854241\n",
      "validation loss:  0.09109634\n",
      "---------- Epoch :15/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0014574898\n",
      "validation loss:  0.09255024\n",
      "---------- Epoch :16/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0008927324\n",
      "validation loss:  0.084003866\n",
      "---------- Epoch :17/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.0005632005\n",
      "validation loss:  0.090353884\n",
      "---------- Epoch :18/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.00047632924\n",
      "validation loss:  0.09765181\n",
      "---------- Epoch :19/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.00041144853\n",
      "validation loss:  0.098996446\n",
      "---------- Epoch :20/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.000359978\n",
      "validation loss:  0.09625348\n",
      "---------- Epoch :21/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.00033188978\n",
      "validation loss:  0.1012153\n",
      "---------- Epoch :22/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.0003071612\n",
      "validation loss:  0.101117425\n",
      "---------- Epoch :23/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.00028749646\n",
      "validation loss:  0.10117028\n",
      "---------- Epoch :24/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.00026059034\n",
      "validation loss:  0.10112976\n",
      "---------- Epoch :25/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.0002413444\n",
      "validation loss:  0.10215506\n",
      "---------- Epoch :26/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.00024343381\n",
      "validation loss:  0.10296012\n",
      "---------- Epoch :27/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.00022891678\n",
      "validation loss:  0.10248198\n",
      "---------- Epoch :28/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.00022766368\n",
      "validation loss:  0.10237893\n",
      "---------- Epoch :29/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.00022653768\n",
      "validation loss:  0.093077704\n",
      "---------- Epoch :30/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.00022239496\n",
      "validation loss:  0.10376664\n",
      "---------- Epoch :31/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.00021622491\n",
      "validation loss:  0.093521334\n",
      "---------- Epoch :32/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.00020450866\n",
      "validation loss:  0.10368476\n",
      "---------- Epoch :33/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.00021741903\n",
      "validation loss:  0.098565705\n",
      "---------- Epoch :34/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.00019978623\n",
      "validation loss:  0.10330032\n",
      "---------- Epoch :35/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.00020310572\n",
      "validation loss:  0.104197375\n",
      "---------- Epoch :36/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.00021659142\n",
      "validation loss:  0.103036866\n",
      "---------- Epoch :37/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.00019688788\n",
      "validation loss:  0.10437401\n",
      "---------- Epoch :38/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.00020181217\n",
      "validation loss:  0.10331623\n",
      "---------- Epoch :39/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.00018464998\n",
      "validation loss:  0.10424378\n",
      "---------- Epoch :40/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.00020195686\n",
      "validation loss:  0.104150295\n",
      "---------- Epoch :41/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.00019270435\n",
      "validation loss:  0.10347468\n",
      "---------- Epoch :42/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.00019846343\n",
      "validation loss:  0.10384507\n",
      "---------- Epoch :43/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.000188424\n",
      "validation loss:  0.10399768\n",
      "---------- Epoch :44/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.00019481844\n",
      "validation loss:  0.10430219\n",
      "---------- Epoch :45/80 ----------\n",
      "learning rate:  1.953125e-05\n",
      "train loss:  0.00019368863\n",
      "validation loss:  0.099139296\n",
      "---------- Epoch :46/80 ----------\n",
      "learning rate:  1.953125e-05\n",
      "train loss:  0.00018969091\n",
      "validation loss:  0.102556735\n",
      "---------- Epoch :47/80 ----------\n",
      "learning rate:  1.953125e-05\n",
      "train loss:  0.00018600216\n",
      "validation loss:  0.10419512\n",
      "---------- Epoch :48/80 ----------\n",
      "learning rate:  1.953125e-05\n",
      "train loss:  0.00019678862\n",
      "validation loss:  0.102369696\n",
      "---------- Epoch :49/80 ----------\n",
      "learning rate:  9.765625e-06\n",
      "train loss:  0.00019354111\n",
      "validation loss:  0.10399782\n",
      "---------- Epoch :50/80 ----------\n",
      "learning rate:  9.765625e-06\n",
      "train loss:  0.00018648188\n",
      "validation loss:  0.103339866\n",
      "---------- Epoch :51/80 ----------\n",
      "learning rate:  9.765625e-06\n",
      "train loss:  0.00019279732\n",
      "validation loss:  0.09904705\n",
      "---------- Epoch :52/80 ----------\n",
      "learning rate:  9.765625e-06\n",
      "train loss:  0.00018963648\n",
      "validation loss:  0.10439019\n",
      "---------- Epoch :53/80 ----------\n",
      "learning rate:  4.8828125e-06\n",
      "train loss:  0.00017888963\n",
      "validation loss:  0.10428593\n",
      "---------- Epoch :54/80 ----------\n",
      "learning rate:  4.8828125e-06\n",
      "train loss:  0.00019211345\n",
      "validation loss:  0.10474174\n",
      "---------- Epoch :55/80 ----------\n",
      "learning rate:  4.8828125e-06\n",
      "train loss:  0.00019475777\n",
      "validation loss:  0.10411618\n",
      "---------- Epoch :56/80 ----------\n",
      "learning rate:  4.8828125e-06\n",
      "train loss:  0.00019954996\n",
      "validation loss:  0.10488945\n",
      "---------- Epoch :57/80 ----------\n",
      "learning rate:  2.44140625e-06\n",
      "train loss:  0.00018514339\n",
      "validation loss:  0.09899523\n",
      "---------- Epoch :58/80 ----------\n",
      "learning rate:  2.44140625e-06\n",
      "train loss:  0.00018458978\n",
      "validation loss:  0.10485061\n",
      "---------- Epoch :59/80 ----------\n",
      "learning rate:  2.44140625e-06\n",
      "train loss:  0.00018545962\n",
      "validation loss:  0.10432404\n",
      "---------- Epoch :60/80 ----------\n",
      "learning rate:  2.44140625e-06\n",
      "train loss:  0.00018375006\n",
      "validation loss:  0.10420958\n",
      "---------- Epoch :61/80 ----------\n",
      "learning rate:  1.220703125e-06\n",
      "train loss:  0.0001874452\n",
      "validation loss:  0.10443367\n",
      "---------- Epoch :62/80 ----------\n",
      "learning rate:  1.220703125e-06\n",
      "train loss:  0.00018641022\n",
      "validation loss:  0.10503599\n",
      "---------- Epoch :63/80 ----------\n",
      "learning rate:  1.220703125e-06\n",
      "train loss:  0.00018192078\n",
      "validation loss:  0.105064094\n",
      "---------- Epoch :64/80 ----------\n",
      "learning rate:  1.220703125e-06\n",
      "train loss:  0.00017928546\n",
      "validation loss:  0.10504605\n",
      "---------- Epoch :65/80 ----------\n",
      "learning rate:  6.103515625e-07\n",
      "train loss:  0.00018155163\n",
      "validation loss:  0.10408496\n",
      "---------- Epoch :66/80 ----------\n",
      "learning rate:  6.103515625e-07\n",
      "train loss:  0.00019781642\n",
      "validation loss:  0.104222104\n",
      "---------- Epoch :67/80 ----------\n",
      "learning rate:  6.103515625e-07\n",
      "train loss:  0.00018341141\n",
      "validation loss:  0.10487744\n",
      "---------- Epoch :68/80 ----------\n",
      "learning rate:  6.103515625e-07\n",
      "train loss:  0.00018157772\n",
      "validation loss:  0.10421116\n",
      "---------- Epoch :69/80 ----------\n",
      "learning rate:  3.0517578125e-07\n",
      "train loss:  0.00018209915\n",
      "validation loss:  0.104422905\n",
      "---------- Epoch :70/80 ----------\n",
      "learning rate:  3.0517578125e-07\n",
      "train loss:  0.00019824393\n",
      "validation loss:  0.1034558\n",
      "---------- Epoch :71/80 ----------\n",
      "learning rate:  3.0517578125e-07\n",
      "train loss:  0.00019054685\n",
      "validation loss:  0.104687475\n",
      "---------- Epoch :72/80 ----------\n",
      "learning rate:  3.0517578125e-07\n",
      "train loss:  0.00018321343\n",
      "validation loss:  0.10402414\n",
      "---------- Epoch :73/80 ----------\n",
      "learning rate:  1.52587890625e-07\n",
      "train loss:  0.00019811677\n",
      "validation loss:  0.10495537\n",
      "---------- Epoch :74/80 ----------\n",
      "learning rate:  1.52587890625e-07\n",
      "train loss:  0.00019385687\n",
      "validation loss:  0.10403694\n",
      "---------- Epoch :75/80 ----------\n",
      "learning rate:  1.52587890625e-07\n",
      "train loss:  0.00018128706\n",
      "validation loss:  0.10484354\n",
      "---------- Epoch :76/80 ----------\n",
      "learning rate:  1.52587890625e-07\n",
      "train loss:  0.0001849401\n",
      "validation loss:  0.10450736\n",
      "---------- Epoch :77/80 ----------\n",
      "learning rate:  7.62939453125e-08\n",
      "train loss:  0.0001947196\n",
      "validation loss:  0.10496616\n",
      "---------- Epoch :78/80 ----------\n",
      "learning rate:  7.62939453125e-08\n",
      "train loss:  0.00019342688\n",
      "validation loss:  0.10443981\n",
      "---------- Epoch :79/80 ----------\n",
      "learning rate:  7.62939453125e-08\n",
      "train loss:  0.00018236619\n",
      "validation loss:  0.10532295\n",
      "---------- Epoch :80/80 ----------\n",
      "learning rate:  7.62939453125e-08\n",
      "train loss:  0.00017323894\n",
      "validation loss:  0.10430935\n",
      "training time:  166.8647  s\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#hyper-parameters values\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(0)\n",
    "\n",
    "num_epochs = 80\n",
    "lr = 0.02\n",
    "weight_decay = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "#model, dataloader, optimizer, scheduler, cost function, early stopping\n",
    "model = VarNet1()\n",
    "model.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,factor=0.5)\n",
    "\n",
    "DataSet = data_set()\n",
    "train_DataSet, val_DataSet = random_split(DataSet,[8000,2000])\n",
    "train_loader = DataLoader(train_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "val_loader = DataLoader(val_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.15)\n",
    "\n",
    "#training loop\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "stop_epoch = num_epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "  print('-'*10,f\"Epoch :{epoch+1}/{num_epochs}\",'-'*10)\n",
    "  print('learning rate: ',optimizer.param_groups[0][\"lr\"])\n",
    "  #train\n",
    "  losses=[]\n",
    "  model.train()\n",
    "  for imgs,labels in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #forward\n",
    "    preds = model(imgs)\n",
    "    #loss and backward\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(preds,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.detach().cpu())\n",
    "  print('train loss: ',np.mean(losses))\n",
    "  train_loss_epochs.append(np.mean(losses))\n",
    "  #validation\n",
    "  val_losses=[]\n",
    "  model.eval()\n",
    "  for imgs,labels in val_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    preds = model(imgs)\n",
    "    val_loss = loss_fn(preds,labels)\n",
    "    val_losses.append(val_loss.detach().cpu())\n",
    "  scheduler.step(np.mean(val_losses))\n",
    "  print('validation loss: ',np.mean(val_losses))\n",
    "  val_loss_epochs.append(np.mean(val_losses))\n",
    "\n",
    "  early_stopping(np.mean(losses), np.mean(val_losses))\n",
    "  if early_stopping.early_stop:\n",
    "    print('-'*10,\" Early stopping \",'-'*10)\n",
    "    stop_epoch = epoch + 1\n",
    "    break\n",
    "train_time = time.time() - startTime\n",
    "print('training time: ',round(train_time,4),' s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwyegqJqLKtN",
    "outputId": "30695c14-2194-4adb-bc0a-59042a7428bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  98.45 %\n",
      "processing time:  0.3424  s\n"
     ]
    }
   ],
   "source": [
    "#tesing\n",
    "startTime = time.time()\n",
    "\n",
    "DataSet = data_set(test=True)\n",
    "len_data = DataSet.__len__()\n",
    "dataloader = DataLoader(DataSet, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "\n",
    "true_preds_tot = 0\n",
    "for imgs,labels in dataloader:\n",
    "  if torch.cuda.is_available():\n",
    "    imgs = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  preds = model(imgs)\n",
    "  true_preds = (torch.max(preds,axis=1).indices == labels).sum()\n",
    "  true_preds_tot += true_preds\n",
    "print('test accuracy: ',100*(true_preds_tot.cpu().numpy()/len_data),'%')\n",
    "test_time = time.time() - startTime\n",
    "print('processing time: ',round(test_time,4),' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iMH9bJ7AF9U"
   },
   "source": [
    "#### Variant 2: Using drop out in conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "566AYaCvAF9j",
    "outputId": "0dac8b73-f317-4a27-831e-4083657a5ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch :1/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  1.0838629\n",
      "validation loss:  0.3014505\n",
      "---------- Epoch :2/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.56846994\n",
      "validation loss:  0.22626108\n",
      "---------- Epoch :3/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.47642356\n",
      "validation loss:  0.2591067\n",
      "---------- Epoch :4/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.48313656\n",
      "validation loss:  0.1763419\n",
      "---------- Epoch :5/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.43608835\n",
      "validation loss:  0.15194291\n",
      "---------- Epoch :6/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.44781515\n",
      "validation loss:  0.17521209\n",
      "---------- Epoch :7/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.45547333\n",
      "validation loss:  0.15991347\n",
      "---------- Epoch :8/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.42504826\n",
      "validation loss:  0.16797191\n",
      "---------- Epoch :9/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.4492943\n",
      "validation loss:  0.14940837\n",
      "---------- Epoch :10/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.39892858\n",
      "validation loss:  0.16971679\n",
      "---------- Epoch :11/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.44340533\n",
      "validation loss:  0.1872374\n",
      "---------- Epoch :12/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.39961374\n",
      "validation loss:  0.21028575\n",
      "---------- Epoch :13/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.42322633\n",
      "validation loss:  0.18277682\n",
      "---------- Epoch :14/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.3325086\n",
      "validation loss:  0.12758236\n",
      "---------- Epoch :15/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.29368466\n",
      "validation loss:  0.11299407\n",
      "---------- Epoch :16/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.27404433\n",
      "validation loss:  0.105743304\n",
      "---------- Epoch :17/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.26356292\n",
      "validation loss:  0.14071885\n",
      "---------- Epoch :18/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.24973685\n",
      "validation loss:  0.111022316\n",
      "---------- Epoch :19/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.25401413\n",
      "validation loss:  0.101674706\n",
      "---------- Epoch :20/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.25125745\n",
      "validation loss:  0.10211876\n",
      "---------- Epoch :21/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.242578\n",
      "validation loss:  0.10984404\n",
      "---------- Epoch :22/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.23863067\n",
      "validation loss:  0.109471\n",
      "---------- Epoch :23/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.2475674\n",
      "validation loss:  0.09042532\n",
      "---------- Epoch :24/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.24265353\n",
      "validation loss:  0.1095884\n",
      "---------- Epoch :25/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.22722481\n",
      "validation loss:  0.10134102\n",
      "---------- Epoch :26/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.21961628\n",
      "validation loss:  0.08907241\n",
      "---------- Epoch :27/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.23744617\n",
      "validation loss:  0.10743026\n",
      "---------- Epoch :28/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.23662508\n",
      "validation loss:  0.102742694\n",
      "---------- Epoch :29/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.22062963\n",
      "validation loss:  0.098602474\n",
      "---------- Epoch :30/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.2263676\n",
      "validation loss:  0.10570505\n",
      "---------- Epoch :31/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.20294619\n",
      "validation loss:  0.07936858\n",
      "---------- Epoch :32/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.19849072\n",
      "validation loss:  0.08500405\n",
      "---------- Epoch :33/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.20125747\n",
      "validation loss:  0.08289323\n",
      "---------- Epoch :34/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.18609098\n",
      "validation loss:  0.08120298\n",
      "---------- Epoch :35/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.18472274\n",
      "validation loss:  0.08227818\n",
      "---------- Epoch :36/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.16135305\n",
      "validation loss:  0.07093451\n",
      "---------- Epoch :37/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.16334452\n",
      "validation loss:  0.07798876\n",
      "---------- Epoch :38/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.15364517\n",
      "validation loss:  0.0781268\n",
      "---------- Epoch :39/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.1556374\n",
      "validation loss:  0.07022441\n",
      "---------- Epoch :40/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.16070645\n",
      "validation loss:  0.07236592\n",
      "---------- Epoch :41/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.14028901\n",
      "validation loss:  0.085328594\n",
      "---------- Epoch :42/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.15633462\n",
      "validation loss:  0.07336243\n",
      "---------- Epoch :43/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.15626244\n",
      "validation loss:  0.07624331\n",
      "---------- Epoch :44/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.14069712\n",
      "validation loss:  0.06960012\n",
      "---------- Epoch :45/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.15711449\n",
      "validation loss:  0.07150068\n",
      "---------- Epoch :46/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.14130984\n",
      "validation loss:  0.07035467\n",
      "---------- Epoch :47/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.14430511\n",
      "validation loss:  0.066349804\n",
      "---------- Epoch :48/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.14250499\n",
      "validation loss:  0.067165144\n",
      "---------- Epoch :49/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.13484189\n",
      "validation loss:  0.065287165\n",
      "---------- Epoch :50/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.12989278\n",
      "validation loss:  0.06964417\n",
      "---------- Epoch :51/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.12759484\n",
      "validation loss:  0.06966533\n",
      "---------- Epoch :52/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.13671151\n",
      "validation loss:  0.0682931\n",
      "---------- Epoch :53/80 ----------\n",
      "learning rate:  0.00125\n",
      "train loss:  0.13193817\n",
      "validation loss:  0.06681354\n",
      "---------- Epoch :54/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.13399464\n",
      "validation loss:  0.066402495\n",
      "---------- Epoch :55/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.13277745\n",
      "validation loss:  0.06695728\n",
      "---------- Epoch :56/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.12446948\n",
      "validation loss:  0.06332613\n",
      "---------- Epoch :57/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.12385363\n",
      "validation loss:  0.069561884\n",
      "---------- Epoch :58/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.1389584\n",
      "validation loss:  0.06634662\n",
      "---------- Epoch :59/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.1209553\n",
      "validation loss:  0.064409845\n",
      "---------- Epoch :60/80 ----------\n",
      "learning rate:  0.000625\n",
      "train loss:  0.12622201\n",
      "validation loss:  0.065449886\n",
      "---------- Epoch :61/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.11677215\n",
      "validation loss:  0.06540547\n",
      "---------- Epoch :62/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.12504391\n",
      "validation loss:  0.065144666\n",
      "---------- Epoch :63/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.12347253\n",
      "validation loss:  0.062271666\n",
      "---------- Epoch :64/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.12267738\n",
      "validation loss:  0.065361984\n",
      "---------- Epoch :65/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.12063108\n",
      "validation loss:  0.06502007\n",
      "---------- Epoch :66/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.12361291\n",
      "validation loss:  0.065521546\n",
      "---------- Epoch :67/80 ----------\n",
      "learning rate:  0.0003125\n",
      "train loss:  0.11822034\n",
      "validation loss:  0.06557943\n",
      "---------- Epoch :68/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.11636468\n",
      "validation loss:  0.06515041\n",
      "---------- Epoch :69/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.1255217\n",
      "validation loss:  0.06485444\n",
      "---------- Epoch :70/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.12750673\n",
      "validation loss:  0.063101634\n",
      "---------- Epoch :71/80 ----------\n",
      "learning rate:  0.00015625\n",
      "train loss:  0.11646639\n",
      "validation loss:  0.065361954\n",
      "---------- Epoch :72/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.116331756\n",
      "validation loss:  0.06431984\n",
      "---------- Epoch :73/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.11464205\n",
      "validation loss:  0.06542598\n",
      "---------- Epoch :74/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.12594447\n",
      "validation loss:  0.06449949\n",
      "---------- Epoch :75/80 ----------\n",
      "learning rate:  7.8125e-05\n",
      "train loss:  0.1201989\n",
      "validation loss:  0.06457663\n",
      "---------- Epoch :76/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.11530754\n",
      "validation loss:  0.062315844\n",
      "---------- Epoch :77/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.12201342\n",
      "validation loss:  0.06492971\n",
      "---------- Epoch :78/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.11806449\n",
      "validation loss:  0.06486698\n",
      "---------- Epoch :79/80 ----------\n",
      "learning rate:  3.90625e-05\n",
      "train loss:  0.123994164\n",
      "validation loss:  0.06410807\n",
      "---------- Epoch :80/80 ----------\n",
      "learning rate:  1.953125e-05\n",
      "train loss:  0.10814987\n",
      "validation loss:  0.065037206\n",
      "training time:  161.2734  s\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#hyper-parameters values\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(1)\n",
    "\n",
    "num_epochs = 80\n",
    "lr = 0.02\n",
    "weight_decay = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "#model, dataloader, optimizer, scheduler, cost function, early stopping\n",
    "model = VarNet2()\n",
    "model.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,factor=0.5)\n",
    "\n",
    "DataSet = data_set()\n",
    "train_DataSet, val_DataSet = random_split(DataSet,[8000,2000])\n",
    "train_loader = DataLoader(train_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "val_loader = DataLoader(val_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.15)\n",
    "\n",
    "#training loop\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "stop_epoch = num_epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "  print('-'*10,f\"Epoch :{epoch+1}/{num_epochs}\",'-'*10)\n",
    "  print('learning rate: ',optimizer.param_groups[0][\"lr\"])\n",
    "  #train\n",
    "  losses=[]\n",
    "  model.train()\n",
    "  for imgs,labels in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #forward\n",
    "    preds = model(imgs)\n",
    "    #loss and backward\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(preds,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.detach().cpu())\n",
    "  print('train loss: ',np.mean(losses))\n",
    "  train_loss_epochs.append(np.mean(losses))\n",
    "  #validation\n",
    "  val_losses=[]\n",
    "  model.eval()\n",
    "  for imgs,labels in val_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    preds = model(imgs)\n",
    "    val_loss = loss_fn(preds,labels)\n",
    "    val_losses.append(val_loss.detach().cpu())\n",
    "  scheduler.step(np.mean(val_losses))\n",
    "  print('validation loss: ',np.mean(val_losses))\n",
    "  val_loss_epochs.append(np.mean(val_losses))\n",
    "\n",
    "  early_stopping(np.mean(losses), np.mean(val_losses))\n",
    "  if early_stopping.early_stop:\n",
    "    print('-'*10,\" Early stopping \",'-'*10)\n",
    "    stop_epoch = epoch + 1\n",
    "    break\n",
    "train_time = time.time() - startTime\n",
    "print('training time: ',round(train_time,4),' s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMEPatSuAF9l",
    "outputId": "295f8a7f-dfbf-4567-bbc6-b49d1c6f8c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  98.35000000000001 %\n",
      "processing time:  0.3268  s\n"
     ]
    }
   ],
   "source": [
    "#tesing\n",
    "startTime = time.time()\n",
    "\n",
    "DataSet = data_set(test=True)\n",
    "len_data = DataSet.__len__()\n",
    "dataloader = DataLoader(DataSet, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "\n",
    "true_preds_tot = 0\n",
    "for imgs,labels in dataloader:\n",
    "  if torch.cuda.is_available():\n",
    "    imgs = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  preds = model(imgs)\n",
    "  true_preds = (torch.max(preds,axis=1).indices == labels).sum()\n",
    "  true_preds_tot += true_preds\n",
    "print('test accuracy: ',100*(true_preds_tot.cpu().numpy()/len_data),'%')\n",
    "test_time = time.time() - startTime\n",
    "print('processing time: ',round(test_time,4),' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnW_Gih1AYs9"
   },
   "source": [
    "#### Variant 3: Using max pool instead of avg pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYrUwqsgAYs-",
    "outputId": "4d4f2395-82dc-4cb9-f910-2b771f41930a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch :1/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.65237755\n",
      "validation loss:  0.21746261\n",
      "---------- Epoch :2/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.22864805\n",
      "validation loss:  0.21179892\n",
      "---------- Epoch :3/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.19685012\n",
      "validation loss:  0.22464813\n",
      "---------- Epoch :4/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.20004572\n",
      "validation loss:  0.20933704\n",
      "---------- Epoch :5/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.17351887\n",
      "validation loss:  0.200209\n",
      "---------- Epoch :6/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.17160477\n",
      "validation loss:  0.21527569\n",
      "---------- Epoch :7/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.16932301\n",
      "validation loss:  0.24762473\n",
      "---------- Epoch :8/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.16940044\n",
      "validation loss:  0.277691\n",
      "---------- Epoch :9/80 ----------\n",
      "learning rate:  0.02\n",
      "train loss:  0.16857508\n",
      "validation loss:  0.218852\n",
      "---------- Epoch :10/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.0847033\n",
      "validation loss:  0.1800688\n",
      "---------- Epoch :11/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.04532944\n",
      "validation loss:  0.18507393\n",
      "---------- Epoch :12/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.036913764\n",
      "validation loss:  0.19692211\n",
      "---------- Epoch :13/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.025615195\n",
      "validation loss:  0.24926952\n",
      "---------- Epoch :14/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.0401113\n",
      "validation loss:  0.24087809\n",
      "---------- Epoch :15/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.0239671\n",
      "validation loss:  0.22171517\n",
      "---------- Epoch :16/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.008446663\n",
      "validation loss:  0.22750546\n",
      "----------  Early stopping  ----------\n",
      "training time:  32.963  s\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#hyper-parameters values\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(0)\n",
    "\n",
    "num_epochs = 80\n",
    "lr = 0.02\n",
    "weight_decay = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "#model, dataloader, optimizer, scheduler, cost function, early stopping\n",
    "model = VarNet3()\n",
    "model.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,factor=0.5)\n",
    "\n",
    "DataSet = data_set()\n",
    "train_DataSet, val_DataSet = random_split(DataSet,[8000,2000])\n",
    "train_loader = DataLoader(train_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "val_loader = DataLoader(val_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.15)\n",
    "\n",
    "#training loop\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "stop_epoch = num_epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "  print('-'*10,f\"Epoch :{epoch+1}/{num_epochs}\",'-'*10)\n",
    "  print('learning rate: ',optimizer.param_groups[0][\"lr\"])\n",
    "  #train\n",
    "  losses=[]\n",
    "  model.train()\n",
    "  for imgs,labels in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #forward\n",
    "    preds = model(imgs)\n",
    "    #loss and backward\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(preds,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.detach().cpu())\n",
    "  print('train loss: ',np.mean(losses))\n",
    "  train_loss_epochs.append(np.mean(losses))\n",
    "  #validation\n",
    "  val_losses=[]\n",
    "  model.eval()\n",
    "  for imgs,labels in val_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    preds = model(imgs)\n",
    "    val_loss = loss_fn(preds,labels)\n",
    "    val_losses.append(val_loss.detach().cpu())\n",
    "  scheduler.step(np.mean(val_losses))\n",
    "  print('validation loss: ',np.mean(val_losses))\n",
    "  val_loss_epochs.append(np.mean(val_losses))\n",
    "\n",
    "  early_stopping(np.mean(losses), np.mean(val_losses))\n",
    "  if early_stopping.early_stop:\n",
    "    print('-'*10,\" Early stopping \",'-'*10)\n",
    "    stop_epoch = epoch + 1\n",
    "    break\n",
    "train_time = time.time() - startTime\n",
    "print('training time: ',round(train_time,4),' s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvAtSRb1AYs_",
    "outputId": "9479052a-df94-4140-e6a0-a9deeaa3737a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  96.95 %\n",
      "processing time:  0.3327  s\n"
     ]
    }
   ],
   "source": [
    "#tesing\n",
    "startTime = time.time()\n",
    "\n",
    "DataSet = data_set(test=True)\n",
    "len_data = DataSet.__len__()\n",
    "dataloader = DataLoader(DataSet, batch_size=batch_size, shuffle=True)\n",
    "model.eval()\n",
    "\n",
    "true_preds_tot = 0\n",
    "for imgs,labels in dataloader:\n",
    "  if torch.cuda.is_available():\n",
    "    imgs = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  preds = model(imgs)\n",
    "  true_preds = (torch.max(preds,axis=1).indices == labels).sum()\n",
    "  true_preds_tot += true_preds\n",
    "print('test accuracy: ',100*(true_preds_tot.cpu().numpy()/len_data),'%')\n",
    "test_time = time.time() - startTime\n",
    "print('processing time: ',round(test_time,4),' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV-b6n0LPJLc"
   },
   "source": [
    "## Training with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC1vkXBPMbCE"
   },
   "source": [
    "#### Variant 4: Using Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2cCglM9WO6WK"
   },
   "outputs": [],
   "source": [
    "#defining data class\n",
    "class data_set(Dataset):\n",
    "    def __init__(self,test=False):\n",
    "        if test:\n",
    "          self.imgs_dir = glob.glob('/content/Reduced MNIST Data/Reduced Testing data/*/*')\n",
    "        else:\n",
    "          self.imgs_dir = glob.glob('/content/Reduced MNIST Data/Reduced Trainging data/*/*')\n",
    "        self.transform_data = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                  transforms.RandomRotation(30),\n",
    "                                                  transforms.RandomResizedCrop(28,(0.9,1)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_dir)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        img_dir = self.imgs_dir[index]\n",
    "        label = int(img_dir.split('/')[-2])\n",
    "        img = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        img = self.transform_data(img)\n",
    "        return img,label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ9pev7lMbCU",
    "outputId": "a978182b-b555-45ef-b1b9-293b21666884",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch :1/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.727655\n",
      "validation loss:  0.32471663\n",
      "---------- Epoch :2/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.26831934\n",
      "validation loss:  0.28205496\n",
      "---------- Epoch :3/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.2154825\n",
      "validation loss:  0.23873456\n",
      "---------- Epoch :4/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.18702193\n",
      "validation loss:  0.19272958\n",
      "---------- Epoch :5/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.17115384\n",
      "validation loss:  0.17173827\n",
      "---------- Epoch :6/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.16619258\n",
      "validation loss:  0.2168139\n",
      "---------- Epoch :7/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.16136166\n",
      "validation loss:  0.18056837\n",
      "---------- Epoch :8/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.1477555\n",
      "validation loss:  0.15273143\n",
      "---------- Epoch :9/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.14464276\n",
      "validation loss:  0.19890048\n",
      "---------- Epoch :10/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.1488507\n",
      "validation loss:  0.16264391\n",
      "---------- Epoch :11/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.13812451\n",
      "validation loss:  0.12721997\n",
      "---------- Epoch :12/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.13646883\n",
      "validation loss:  0.15390845\n",
      "---------- Epoch :13/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.13870884\n",
      "validation loss:  0.16740726\n",
      "---------- Epoch :14/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.13945714\n",
      "validation loss:  0.16230324\n",
      "---------- Epoch :15/80 ----------\n",
      "learning rate:  0.01\n",
      "train loss:  0.12549457\n",
      "validation loss:  0.13846435\n",
      "---------- Epoch :16/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.086814985\n",
      "validation loss:  0.11871466\n",
      "---------- Epoch :17/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.07582145\n",
      "validation loss:  0.12435028\n",
      "---------- Epoch :18/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.08271881\n",
      "validation loss:  0.13595068\n",
      "---------- Epoch :19/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.08169704\n",
      "validation loss:  0.10007823\n",
      "---------- Epoch :20/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.078955986\n",
      "validation loss:  0.13633202\n",
      "---------- Epoch :21/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.08041171\n",
      "validation loss:  0.11385921\n",
      "---------- Epoch :22/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.078861825\n",
      "validation loss:  0.10186122\n",
      "---------- Epoch :23/80 ----------\n",
      "learning rate:  0.005\n",
      "train loss:  0.08182841\n",
      "validation loss:  0.120598696\n",
      "---------- Epoch :24/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.06323485\n",
      "validation loss:  0.11279324\n",
      "---------- Epoch :25/80 ----------\n",
      "learning rate:  0.0025\n",
      "train loss:  0.044974767\n",
      "validation loss:  0.11055044\n",
      "----------  Early stopping  ----------\n",
      "training time:  143.4691  s\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "#hyper-parameters values\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(0)\n",
    "\n",
    "num_epochs = 80\n",
    "lr = 0.01\n",
    "weight_decay = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "#model, dataloader, optimizer, scheduler, cost function, early stopping\n",
    "model = LeNet5()\n",
    "model.apply(initialize_weights)\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,factor=0.5)\n",
    "\n",
    "DataSet = data_set()\n",
    "train_DataSet, val_DataSet = random_split(DataSet,[8000,2000])\n",
    "train_loader = DataLoader(train_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "val_loader = DataLoader(val_DataSet, batch_size=batch_size, shuffle=True, drop_last=True, generator=g)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.05)\n",
    "\n",
    "#training loop\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = []\n",
    "stop_epoch = num_epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "  print('-'*10,f\"Epoch :{epoch+1}/{num_epochs}\",'-'*10)\n",
    "  print('learning rate: ',optimizer.param_groups[0][\"lr\"])\n",
    "  #train\n",
    "  losses=[]\n",
    "  model.train()\n",
    "  for imgs,labels in train_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    #forward\n",
    "    preds = model(imgs)\n",
    "    #loss and backward\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(preds,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.detach().cpu())\n",
    "  print('train loss: ',np.mean(losses))\n",
    "  train_loss_epochs.append(np.mean(losses))\n",
    "  #validation\n",
    "  val_losses=[]\n",
    "  model.eval()\n",
    "  for imgs,labels in val_loader:\n",
    "    if torch.cuda.is_available():\n",
    "      imgs = imgs.cuda()\n",
    "      labels = labels.cuda()\n",
    "    preds = model(imgs)\n",
    "    val_loss = loss_fn(preds,labels)\n",
    "    val_losses.append(val_loss.detach().cpu())\n",
    "  scheduler.step(np.mean(val_losses))\n",
    "  print('validation loss: ',np.mean(val_losses))\n",
    "  val_loss_epochs.append(np.mean(val_losses))\n",
    "\n",
    "  early_stopping(np.mean(losses), np.mean(val_losses))\n",
    "  if early_stopping.early_stop:\n",
    "    print('-'*10,\" Early stopping \",'-'*10)\n",
    "    stop_epoch = epoch + 1\n",
    "    break\n",
    "train_time = time.time() - startTime\n",
    "print('training time: ',round(train_time,4),' s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TvS-zY-MbCV",
    "outputId": "376022a7-3ccc-4ff5-f2fa-e989f4a92196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  97.75 %\n",
      "processing time:  1.0574  s\n"
     ]
    }
   ],
   "source": [
    "#tesing\n",
    "startTime = time.time()\n",
    "\n",
    "g = set_seed(0)\n",
    "\n",
    "DataSet = data_set(test=True)\n",
    "len_data = DataSet.__len__()\n",
    "dataloader = DataLoader(DataSet, batch_size=batch_size, shuffle=True, generator=g)\n",
    "model.eval()\n",
    "\n",
    "true_preds_tot = 0\n",
    "for imgs,labels in dataloader:\n",
    "  if torch.cuda.is_available():\n",
    "    imgs = imgs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  preds = model(imgs)\n",
    "  true_preds = (torch.max(preds,axis=1).indices == labels).sum()\n",
    "  true_preds_tot += true_preds\n",
    "print('test accuracy: ',100*(true_preds_tot.cpu().numpy()/len_data),'%')\n",
    "test_time = time.time() - startTime\n",
    "print('processing time: ',round(test_time,4),' s')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "V3_gsrJlqThn",
    "509IoijzKDeo",
    "oHzWSJkaLLV5",
    "y7a--0_S-brI",
    "8iMH9bJ7AF9U",
    "hC1vkXBPMbCE"
   ],
   "name": "Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
